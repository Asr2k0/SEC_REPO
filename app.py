import ast
from langchain.memory import ConversationBufferWindowMemory
import re
import email_sender
import misc_variables
import news_scrapper
import prompts
from news_scrapper import *
from langchain.chat_models import ChatOpenAI
from langchain.memory import ConversationBufferMemory
from langchain.prompts import PromptTemplate
from langchain.chains import LLMChain
import streamlit as st
from langchain_core.output_parsers import StrOutputParser
from langchain.chains.question_answering import load_qa_chain
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough
import os
import warnings
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import Chroma
from email_sender import *
from dotenv import load_dotenv
from misc_variables import *
load_dotenv()
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
# conversational agent memory Only for streamlit because streamlit runs from top to down
if 'memory' not in st.session_state:

    st.session_state.memory = ConversationBufferWindowMemory(
        memory_key='chat_history',
        k=10,
        return_messages=True
    )

master_comapnies = misc_variables.master_comapnies
company_name_to_code = misc_variables.company_name_to_code
available_reports = misc_variables.available_reports



def get_chroma_store():
    """

    This function retrieves the locally available vector store 'chroma_db'.
    It initializes an embedding function (OpenAIEmbeddings) and uses it to
    set up the vector store, which is stored in the './chroma_db' directory. '''
    :return: vectorstore

    """
    try :
        embedding_function = OpenAIEmbeddings()
        vectorstore = Chroma(persist_directory='./chroma_db', embedding_function=embedding_function)
        return vectorstore
    except Exception as e:
        print(f'Error in get_chroma_store the error is {e}')


def get_chat_response(question):
    '''
    This is the core function that handles generating a chat response based on user input.
    It takes in a question,  as input, then retrieves the
    relevant vector store and uses the agentic_rag_handler to generate the output.
    '''
    try:
        company_name = st.session_state.selected_company
        year = st.session_state.selected_year
        report_type = st.session_state.selected_report_type
        vectorstore = get_chroma_store()
        return agentic_rag_handler(question, vectorstore, company_name, year, report_type)
    except Exception as e:
        print(f'Error in get_chat_response the error is {e}')

def data_retrival_response_generator(question, vectorstore, query, report_type, company_name, year, perf_cmp=False):
    '''
    This tool is the main component responsible for retrieving relevant documents and generating a response based on the question.
    It performs filtering of data based on company name, year, report type (10-Q or 10-K), and an optional performance comparison flag (perf_cmp).
    The function interacts with the vector store to fetch the most relevant documents and then generates a response using a language model (LLM).

    Parameters:
    - question: The user's query for which a response needs to be generated.
    - vectorstore: The vector store (Chroma or similar) containing the document embeddings.
    - query: The query to retrieve relevant documents (not used in this function directly, possibly for future extension).
    - report_type: The type of report, either "10-Q" or "10-K".
    - company_name: The name of the company for which the report is to be retrieved.
    - year: The year for which the report is to be retrieved.
    - perf_cmp (optional): A boolean flag to indicate whether performance comparison is required. Defaults to False.

    Returns:
    - A response generated by the LLM based on the relevant documents retrieved from the vector store.
    '''
    try:
        llm = ChatOpenAI()
        company_code = company_name_to_code.get(company_name)
        if perf_cmp == False:
            template = prompts.data_retrival_response_generator_prompt
            if report_type == "10-Q":
                # Fetch all documents for the specified company and year, ignoring form_type
                report_type = "10Q"
                metadata_filter = {
                    "$and": [
                        {"company_name": {"$eq": company_code}},
                        {"year": {"$eq": str(year)}}
                    ]
                }
            else:
                report_type = "10K"
                metadata_filter = {
                    "$and": [
                        {"company_name": {"$eq": company_code}},
                        {"year": {"$eq": str(year)}},
                        {"form_type": {"$eq": report_type}}
                    ]
                }

        elif perf_cmp == True:
            template = prompts.data_retrival_response_generator_prompt_cmp
            if report_type == "10Q" or report_type == "10-Q":
                report_type = "10Q"
                metadata_filter = {
                    "$and": [
                        {"company_name": {"$eq": company_code}},
                        {"year": {"$eq": str(year)}}
                    ]
                }
            else:
                if report_type == "10K" or report_type == "10-K":
                    report_type = "10K"
                    metadata_filter = {
                        "$and": [
                            {"company_name": {"$eq": company_code}},
                            {"form_type": {"$eq": report_type}}
                        ]
                    }

        retriever = vectorstore.as_retriever(search_kwargs={"filter": metadata_filter}, k=5)
        prompt = ChatPromptTemplate.from_template(template)
        # print(prompt)
        chain = (

                {"context": retriever,
                 "query": RunnablePassthrough()}
                | prompt | llm | StrOutputParser()

        )

        return chain.invoke(question)
    except Exception as e:
        print(f'Error in data_retrival_response_generator and the error is {e} ')

# Tool
def perfomance_calculator(question, query):
    '''
    This tool is responsible for calculating performance based on a given question and query.


    Parameters:
    - question: The question provided by the user related to performance.
    - query: The query or data summary that provides the context for performance calculation.

    Returns:
    - The result of the performance calculation, which is processed and returned as a string output
    '''
    try:
        template = prompts.calculator_prompt


        llm = ChatOpenAI()
        prompt = ChatPromptTemplate.from_template(template)

        chain = (
                prompt |  # Apply the prompt template
                llm |  # Send to the LLM for processing
                StrOutputParser()  # Parse the LLM output as a string
        )

        # Return the processed result by invoking the chain with the query
        return chain.invoke({"question": question, "summary": query})
    except Exception as e:
        print(f'Error occured in perfomance_calculator and the error is {e}')

#tool
def email_tool(emails_list, memory=st.session_state.memory):
    '''

    This tool is to send email the LLM automatically summarises the memory so far
    :param emails_list:
    :param memory:
    :return: success whether email is sent or not
    '''
    try:

        memory_variables = memory.load_memory_variables({})
        messages = memory_variables['chat_history']

        convo_history = " "
        print("Entering Email loop")

        # Concatenate message content
        for message in messages:
            convo_history += message.content

        # Now we format the prompt using the convo_history
        prompt = prompts.email_prompt_template
        prompt = prompt.format(convo_history=convo_history)  # Format the prompt with the convo history


        llm = ChatOpenAI()
        response = llm.predict(prompt)
        print(response)
        subject = "SUMMARY OF YOUR CONVERSATION WITH SEC BOT"
        return email_sender.send_email(subject=subject,body=response,to_emails=emails_list)
    except Exception as e :
        print(f'Error occured in sending email and the error is {e}')







# @st.cache_data(show_spinner=False)
# Our main brain
def agentic_rag_handler(question, _vectorstore, company_name, year, report_type,memory = st.session_state.memory):
    '''

     This function handles the core logic for processing user queries in a report-generation system,
    which uses retrieval-augmented generation (RAG) to fetch relevant data from a vector store,
    perform performance calculations, or send emails based on user input.

    :param question: The user query
    :param _vectorstore: The vector store
    :param company_name: Company name
    :param year: year of the report
    :param report_type: Filling type
    :param memory: ConversationBufferWindowMemory()
    :return: Decide the appropriate tool to use.
    '''
    try:
        prompt = prompts.agentic_rag_prompt
        question_lower = question.lower()
        if not re.search(rf'\b{re.escape(company_name.lower())}\b', question_lower):

            detected_company = next((company for company in master_comapnies if company in question_lower), None)


            if detected_company:
                return "Please ask about the selected comapny and not the other Company"

        if not re.search(rf'\b{re.escape(company_name)}\b', question, re.IGNORECASE):
            question += f'for {company_name}'
        print ("Finished file processing ")
        prompt_template = PromptTemplate(template=prompt, input_variables=["question", "chat_history", "year"])

        # print("JI")

        llm = ChatOpenAI()


        tool_selector_chain = LLMChain(llm=llm, prompt=prompt_template, memory=memory, verbose=False)
        result = tool_selector_chain.run(question)
        # print(result)


        try:

            result = ast.literal_eval(result)
            #print(result)
            tool = result[0]
            print("tool is " ,tool)
            if tool == "Retrieve":
                queries = result[1:]
                print(queries)
                response = " "
                quarter_pattern = r"(Q[1-4])(?!\s*\d{4})"  # Match Q1, Q2, Q3, or Q4 with no year following it
                for query in queries:
                    if re.search(quarter_pattern, query):

                        query = re.sub(quarter_pattern, r"\1 " + str(year), query)

                    query_response = data_retrival_response_generator(query, _vectorstore, query, report_type, company_name,
                                                                      year)
                    if query_response == "Sorry Not Sure.":
                        pass
                    else:
                        response += f'\n {query_response}'
                #     print(query_response)
                # print(response)
                try:
                    news_str = '\n \n'
                    news_str += f' \n  Since you are finding information about {company_name} here are some news articles which might intrest you :\n'
                    # if company_name == "E"
                    # print(company_name)
                    news_list = news_scrapper.news_helper(company_name)
                    for news in news_list:
                        news_str += f'''
                            \n Title :  {news[0]} \n 
                            \n Link : {news[1]}\n 
            
                        '''

                    response += news_str
                except:
                    pass
                # print("News not coming")
            elif tool == "Calculator":
                queries = result[1:]
                response = " "
                quarter_pattern = r"(Q[1-4])(?!\s*\d{4})"
                for query in queries:
                    if re.search(quarter_pattern, query):
                        # Add the year after the quarter term (e.g., "Q1" becomes "Q1 2023")
                        query = re.sub(quarter_pattern, r"\1 " + str(year), query)
                    print(query)
                    query_response = data_retrival_response_generator(query, _vectorstore, question, report_type,
                                                                      company_name, year, perf_cmp=False)
                    # print(query_response)
                    if query_response == "Sorry Not Sure.":
                        pass
                    else:
                        response += f'/n {query_response}'
                    print(query_response)

                response = perfomance_calculator(question, response)



            elif tool == "Email":
                emails_list = result[1:]
                #
                # # emails_list = ["secfilingbotgenai@gmail.com"]
                # status = email_tool(emails_list,memory=st.session_state.memory)

                status = email_tool(emails_list, memory=st.session_state.memory)
                if status == 200:
                    response = "Sent the Emails"
                elif status!=200:

                    response = "Sending Error in one of the emails , try again later!"

            memory.save_context({"input": question}, {"output": response})

        except Exception as e :
            print(f"Error in agent handler and the error is {e}")
            response = "Hello , How can I help you today? "
        return response
        # return conversation_chain.run(question)
    except Exception as e :
        print(f'Exception occured in the agentic handler and the exception is {e}')


def main():
    '''

    This is the core logic for the streamlit based UI
    :return:
    '''
    # Page setup
    st.set_page_config(page_title="Chat with SEC Filings", page_icon="ðŸ“„")
    st.title("Chat with SEC Filings ðŸ“„")

    # Create columns for dropdowns
    col1, col2, col3 = st.columns([1, 1, 2])

    # Dropdown filters for company name, year, and report type
    with col1:
        company_name = st.selectbox("Select Company", [None, "Amazon","American Express","Boeing","Johnson and Johnson","Eli Lily","Meta","Tesla", "Netflix", "P&G","Pfizer"])
        print(company_name)
    with col2:
        year = st.selectbox("Select Year", [None, "2022", "2023", "2024"])

    with col3:
        company_code = company_name_to_code.get(company_name) if company_name else None
        print(company_code)
        print(available_reports.get(company_code))
        available_report_types = available_reports.get(company_code, {}).get(year, []) if company_code and year else []
        print(available_report_types)
        report_type = st.selectbox("Select Report Type", [None] + available_report_types)
    print(st.session_state)

    print(st.session_state)
    # Check if selections are valid and proceed
    if company_name and year and report_type:
        # Set selected values in session state for use in the backend
        st.session_state.selected_company = company_name
        st.session_state.selected_year = year
        st.session_state.selected_report_type = report_type

        # Chat functionality
        st.subheader("Ask about the SEC filings")

        # Initialize chat history if not already initialized
        if "messages" not in st.session_state:
            st.session_state.messages = []

        # Display conversation history in a chat-like format
        for message in st.session_state.messages:
            if message["role"] == "user":
                with st.chat_message("user"):
                    st.markdown(message["content"])
            else:
                with st.chat_message("assistant"):
                    st.markdown(message["content"])

        # User input field
        user_input = st.chat_input("Ask a question about the filings:")
        if user_input:
            st.session_state.messages.append({"role": "user", "content": user_input})

            # Display user message immediately
            with st.chat_message("user"):
                st.markdown(user_input)

            # Get response (simulate backend logic)
            response = get_chat_response(user_input)
            response = response.replace("$", " \$")
            print(user_input, response)

            # Append the assistant's response
            st.session_state.messages.append({"role": "assistant", "content": response})

            # Display assistant's response
            with st.chat_message("assistant"):
                st.markdown(response)
    else:

        st.write("Please select a valid company, year, and report type to start chatting.")



if __name__ == "__main__":


    main()
